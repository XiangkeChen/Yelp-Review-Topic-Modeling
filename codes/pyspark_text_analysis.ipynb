{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1575516251452_0001</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-63-105.ec2.internal:20888/proxy/application_1575516251452_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-48-127.ec2.internal:8042/node/containerlogs/container_1575516251452_0001_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<SparkContext master=yarn appName=livy-session-0>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f5c3dab8b00>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "#import pandas as pd\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.json('s3://trendsyelp/review.json')\n",
    "\n",
    "df = spark.read.option('inferSchema','true').format('json').load('s3://trendsyelp/review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# show schema\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[business_id: string, cool: bigint, date: string, funny: bigint, review_id: string, stars: double, text: string, useful: bigint, user_id: string]"
     ]
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations and \n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "def remove_punct(text):\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  \n",
    "    return nopunct\n",
    "\n",
    "def convert_rating(rating):\n",
    "    if rating >=4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "punct_remover = udf(lambda x: remove_punct(x))\n",
    "rating_convert = udf(lambda x: convert_rating(x))\n",
    "\n",
    "df_text = df.select('business_id', 'date', punct_remover('text'), rating_convert('stars')).limit(100)\n",
    "\n",
    "\n",
    "df_text = df_text.withColumnRenamed('<lambda>(text)', 'text')\n",
    "df_text = df_text.withColumnRenamed('<lambda>(stars)', 'rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+----+\n",
      "|         business_id|               date|                text|rate|\n",
      "+--------------------+-------------------+--------------------+----+\n",
      "|ujmEBvifdJM6h6RLv...|2013-05-07 04:34:36|Total bill for th...|   0|\n",
      "|NZnhc2sEQy3RmzKTZ...|2017-01-14 21:30:33|I  adore  Travis ...|   1|\n",
      "|WTqjgwHlXbSFevF32...|2016-11-09 20:09:03|I have to say tha...|   1|\n",
      "|ikCg8xy5JIg_NGPx-...|2018-01-09 20:56:38|Went in for a lun...|   1|\n",
      "|b1b1eb3uo-w561D0Z...|2018-01-30 23:07:38|Today was my seco...|   0|\n",
      "|eU_713ec6fTGNO4Be...|2013-01-20 13:25:59|I ll be the first...|   1|\n",
      "|3fw2X5bZYeW9xCz_z...|2016-05-07 01:21:02|Tracy dessert had...|   0|\n",
      "|zvO-PJCpNk4fgAVUn...|2010-10-05 19:12:35|This place has go...|   0|\n",
      "|b2jN2mm9Wf3RcrZCg...|2015-01-18 14:04:18|I was really look...|   0|\n",
      "|oxwGyA17NL6c5t1Et...|2012-02-29 21:52:43|It s a giant Best...|   0|\n",
      "|8mIrX_LrOnAqWsB5J...|2011-11-30 02:11:15|Like walking back...|   1|\n",
      "|mRUVMJkUGxrByzMQ2...|2017-12-15 23:27:08|Walked in around ...|   0|\n",
      "|FxLfqxdYPA6Z85PFK...|2016-05-07 01:36:53|Wow  So surprised...|   1|\n",
      "|LUN6swQYa4xJKaM_U...|2018-04-27 20:25:26|Michael from Red ...|   1|\n",
      "|AakkkTuGZA2KBodKi...|2012-07-16 00:37:14|I cannot believe ...|   0|\n",
      "|YvrylyuWgbP90RgMq...|2017-04-07 21:27:49|You can t really ...|   1|\n",
      "|NyLYY8q1-H3hfsTwu...|2015-01-03 22:47:34|Great lunch today...|   1|\n",
      "|cHdJXLlKNWixBXpDw...|2015-04-01 16:30:00|I love chinese fo...|   0|\n",
      "|6lj2BJ4tJeu7db5as...|2017-05-26 01:23:19|We ve been a huge...|   1|\n",
      "|y-Iw6dZflNix4BdwI...|2014-06-27 21:19:23|Good selection of...|   0|\n",
      "+--------------------+-------------------+--------------------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[business_id: string, date: string, text: string, rate: string]"
     ]
    }
   ],
   "source": [
    "df_text.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tolenize the text\n",
    "\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "#tokenizer and stop word remover\n",
    "tok = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "#stop word remover\n",
    "stopwordrm = StopWordsRemover(inputCol='words', outputCol='words_nsw')\n",
    "# Build the pipeline \n",
    "pipeline = Pipeline(stages=[tok, stopwordrm])\n",
    "# Fit the pipeline \n",
    "review_tokenized = pipeline.fit(df_text).transform(df_text).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|         business_id|               date|                text|rate|               words|           words_nsw|\n",
      "+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|lPljcKGwLlcwkLT5c...|2011-08-29 17:12:47|The staff pretty ...|   0|[the, staff, pret...|[staff, pretty, m...|\n",
      "|N0apJkxIem2E8irTB...|2011-07-05 01:11:33|We made the mista...|   1|[we, made, the, m...|[made, mistake, m...|\n",
      "|XqJG7Ux_mMfMJnyG2...|2018-05-10 18:54:20|I cant give a ful...|   1|[i, cant, give, a...|[cant, give, full...|\n",
      "|JjupAcmP7-ytPG49j...|2016-02-14 18:45:16|Un service impecc...|   1|[un, service, imp...|[un, service, imp...|\n",
      "|jOE3XbP3TLPBC8-tv...|2016-10-30 17:36:18|I went to Carolyn...|   1|[i, went, to, car...|[went, carolyn, f...|\n",
      "|OQ2QdaD0StfptJXv1...|2018-08-27 01:57:44|The place is diff...|   1|[the, place, is, ...|[place, different...|\n",
      "|K6AKMjAdCSe8xllsb...|2017-05-28 23:11:57|This is the best ...|   1|[this, is, the, b...|[best, bubble, te...|\n",
      "|pH0BLkL4cbxKzu471...|2010-01-21 01:12:18|So this was my  n...|   0|[so, this, was, m...|[, nd, time, , st...|\n",
      "|yvqMh9kTv3Kx-tocc...|2015-11-22 18:49:56|Very disappointed...|   0|[very, disappoint...|[disappointed, we...|\n",
      "|IMLrj2klosTFvPRLv...|2017-02-11 23:46:45|The decor and con...|   1|[the, decor, and,...|[decor, concept, ...|\n",
      "|7Ny2GHLw8xl_kXfhf...|2014-04-24 15:24:18|Gotta say  I was ...|   0|[gotta, say, , i,...|[gotta, say, , bi...|\n",
      "|EJs6fYFdecCjXq8pC...|2017-01-31 01:48:32|Don t waste your ...|   0|[don, t, waste, y...|[waste, time, unl...|\n",
      "|wQ7toM7-hLmVgl1JX...|2015-09-27 23:35:24|We used Spay Neut...|   1|[we, used, spay, ...|[used, spay, neut...|\n",
      "|rAOmFTHze1n-xuBxR...|2011-03-26 18:44:43|My first clubbing...|   1|[my, first, clubb...|[first, clubbing,...|\n",
      "|YL-ZC82ChlQkkXajN...|2013-10-15 22:00:43|This is always an...|   1|[this, is, always...|[always, interest...|\n",
      "|SieL_rcBX-azp4bDL...|2013-03-21 19:17:59|Had breakfast wit...|   1|[had, breakfast, ...|[breakfast, son, ...|\n",
      "|1HD5iUUfVJDbfEBIn...|2012-09-17 06:09:49|typical boba plac...|   0|[typical, boba, p...|[typical, boba, p...|\n",
      "|nQD284VfImPe_UXkm...|2015-10-11 20:50:03|I used a Groupon ...|   1|[i, used, a, grou...|[used, groupon, b...|\n",
      "|I4Nr-MVc26qWr08-S...|2014-11-01 20:42:22|The stars are for...|   1|[the, stars, are,...|[stars, food, , ,...|\n",
      "|PPFcjkmg8DlU4AL85...|2014-07-16 16:37:52|Came here for a c...|   0|[came, here, for,...|[came, cone, dinn...|\n",
      "+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "review_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri gram\n",
    "\n",
    "ngram = NGram(inputCol = 'words', outputCol = 'ngram', n = 3)\n",
    "add_ngram = ngram.transform(review_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer and tf-idf\n",
    "\n",
    "cv_ngram = CountVectorizer(inputCol='ngram', outputCol='tf_ngram')\n",
    "cvModel_ngram = cv_ngram.fit(add_ngram)\n",
    "cv_df_ngram = cvModel_ngram.transform(add_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF matrix\n",
    "\n",
    "idf_ngram = IDF().setInputCol('tf_ngram').setOutputCol('tfidf_ngram')\n",
    "tfidfModel_ngram = idf_ngram.fit(cv_df_ngram)\n",
    "tfidf_df_ngram = tfidfModel_ngram.transform(cv_df_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "splits_ngram = tfidf_df_ngram.select(['tfidf_ngram', 'rate']).randomSplit([0.8,0.2],seed=100)\n",
    "train_ngram = splits_ngram[0].cache()\n",
    "test_ngram = splits_ngram[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature matrix to vectors\n",
    "\n",
    "train_lb_ngram = train_ngram.rdd.map(lambda row: LabeledPoint(row[1], Vectors.fromML(row[0])))\n",
    "test_lb_ngram = test_ngram.rdd.map(lambda row: LabeledPoint(row[1], Vectors.fromML(row[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "numIterations = 50\n",
    "regParam = 0.3\n",
    "svm = SVMWithSGD.train(train_lb_ngram, numIterations, regParam=regParam)\n",
    "\n",
    "#test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))\n",
    "scoreAndLabels_test = test_lb_ngram.map(lambda x: (float(svm.predict(x.features)), x.label))\n",
    "score_label_test = spark.createDataFrame(scoreAndLabels_test, [\"prediction_rate\", \"actual_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|prediction_rate|actual_rate|\n",
      "+---------------+-----------+\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        0.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        0.0|\n",
      "|            1.0|        0.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        0.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            0.0|        0.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            1.0|        1.0|\n",
      "|            0.0|        0.0|\n",
      "+---------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "score_label_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7925"
     ]
    }
   ],
   "source": [
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"actual_rate\", predictionCol=\"prediction_rate\", metricName=\"f1\")\n",
    "svm_f1 = f1_eval.evaluate(score_label_test)\n",
    "print(\"F1 score: %.4f\" % svm_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
